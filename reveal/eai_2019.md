# Embedded Analog Intelligence
### Embedded systems interface with analog sensors and process information in real-time at the edge with advanced AI algorithms for increased autonomy

Notes: Examples notes

---

<!-- .slide: data-background-iframe="EAI.logo"-->

## Introduction

* From traditional industrial robotic systems to today’s latest collaborative robots (or “cobots”), robots rely artificial intelligence (AI) to become “autonomous,” making real-time decisions at the edge.

* This session will discuss "embedded analog intelligence" which is how to benefit from latest sensors, sensor fusion, and most recent deep neural networks to take the best real time decision at the edge.

* Lecture : 
    - language : English 
    - Duration: 30mn

Notes: 
* AI is a component of modern robot

* Key differentiation is:
    - Embedded, real-time
    - Analog sensing and actuation
    - Intelligent / leading edge processing

Embedded Analog Intelligence

---

## Historical point of view

----

## First robot: Unimate a “programmed article transfer”

|UNIMATE PUMA 200 Robot Arm| Patents |
|--|--|
|![]() | ![]() |
| > Picture by Razor Robotics licensed under the Creative Commons Attribution 2.0 Generic on Wikimedia | > Source: https://patents.google.com/patent/US2988237A/en
|
 

Notes: 
George Devol was only 9 years old when the word “robot” first appeared, in 1921, introduced in Karel Capek’s play <a href="http://en.wikipedia.org/wiki/R.U.R._%28Rossum%27s_Universal_Robots%29">R.U.R. (Rossum’s Universal Robots)</a>

The robots in the play had a human form and were manufactured in vats like beer. In contrast, the robots that Devol would invent decades later were electromechanical machines—the first digitally operated programmable robotic arms—and they would start a revolution in manufacturing that continues to this day.</p>   <p><a href="http://en.wikipedia.org/wiki/George_Devol">Devol</a>, who died August 11, 2011, at the age 99, was a prolific inventor and entrepreneur. His work led to the development of the first industrial robot, called Unimate [photo above],

<p>In fact, in 1941 Isaac Asimov coined the word “robotics” in his story “<a href="http://en.wikipedia.org/wiki/Liar!">Liar!</a>” in <em>Astounding Science Fiction</em> magazine. Asimov told me in great detail when we met one evening in New York why he coined the word. He was tired of listing all the activities around robots such as design, construction, operations, manufacturing, etc. He wanted a word to cover all of this. He did not know at the time that Devol was already the living embodiment of robotics.</p>

<p>In 1954, Devol applied for a patent for a device called the Programmed Article Transfer. Looking for an entrepreneurial partner, Devol found one, at a cocktail party, by the name of <a href="http://en.wikipedia.org/wiki/Joseph_Engelberger">Joseph Engelberger</a>, an executive with engineering degrees from Columbia University. Engelberger, who shared an enthusiasm for science fiction with Devol, took the transfer machine to his heart, he told me during an interview in 1977.</p>   <p><img alt="george devol patent" height="343" src="/image/1929980" width="450"></p>   <p><img alt="george devol unimation unimate" height="377" src="/image/1929930" width="450"></p>   <p>Their device morphed from “programmed article transfer” to “manipulator” to “robot.” Devol and Engelberger made this decision to help improve their marketing opportunities. Selling the concept even with a working prototype was an uphill chore. But it paid off:&nbsp;The robot connection gave the project an extra dose of energy that helped it succeed.</p>   <p>The first Unimate, a product of their new Unimation Corp., was hydraulically powered. Its control system relied upon digital control, a magnetic drum memory, and discrete solid-state control components. In 1961 the first Unimate was installed at a GM plant in Trenton, New Jersey, to assist a hot die-casting machine. Unimation would soon develop robots for welding and other applications. <a href="http://www.google.com/patents?id=nBlQAAAAEBAJ&amp;zoom=4&amp;pg=PA13#v=onepage&amp;q&amp;f=false">Patent Number 2,988,237</a> was the seed that spawned the robot industry.</p> 
https://patents.google.com/patent/US2988237A/en

----

## First robot sensors : Electromecanical sensors

| Temp | Position |
|------|----------|
| ![]() | ![]()   |
| Fig 1. | Fig 2. |

Notes: The principle behind a bimetallic strip is that different metals expand to different extents with temperature changes. By combining two different metals one on top of another into a strip, a bimetallic strip is formed. As the two metals expand or contract differently under the same temperature change, the strip bends. It can then be used to switch on or off a circuit at certain temperatures. Bimetallic strips are often found in ovens. The typical structure of this type of control is shown in Fig 1.

Fig 2. illustrate a rotary switch A rotary switch is a switch operated by rotation.  rotary switch consists of a spindle or "rotor" that has a contact arm or "spoke" which projects from its surface like a cam. It has an array of terminals, arranged in a circle around the rotor, each of which serves as a contact for the "spoke" through which any one of a number of different electrical circuits can be connected to the rotor. 

https://blog.robotiq.com/the-history-of-robot-programming-languages
Programming: 
he first real "robotic programming" arrived, unsurprisingly, along with the first industrial robot. The Unimate was invented by George Devol in 1954, who you can learn more about in our previous article. The robot used a very low level of programming (as described in the original patent), which involved "teaching" the robot. The user commanded the robot to a position then stored the positions of all the joints. When the program was played back, the joints would move between positions comparing them to its current position. There was no "language" as such, but this process still happens at the low level of modern robot programming languages.

---

## Today's robots

----

## Today's robot - Cambrian explosion

| Public | Research & Education | Industrial | Commercial | Consumer | 
|--------|-----------------------|----------|-------------|----------|
| ![]() |  ![]() |  ![]() |  ![]() |  ![]() | 
| Defense |  K12 |  Manufacturing/Collaborative | Last mile delivery (wheeled/flying) |  Home & Lawn | 
| Security/Surveillance |  College University | Construction/Demolition |  Inventory/wholesale/retail |  Toy & Hobby | 
| Public safey |  Vocational training |  Oil and Gas |  Hospitality/ Healthcare QoL |  Home Healthcare QoL | 
| Emergency response |  Research |  Mining and Quarrying |  Wharehouse/Distribution |  Social Ententairnment | 
| Ressource Management |  Exploration |  Agriculture |  Tele Presence |  Transportation | 

Notes:
Source: WTWH Media LLC webcast + own research
Key message: after years of increasing penetration – 

https://ifr.org/downloads/press2018/Executive_Summary_WR_2018_Industrial_Robots.pdf
30% CAGR according to IFR
Since 2010, the automotive industry –the most important customer of industrial robots– considerably increased investments in industrial robots worldwide

Robot sales to the electrical/electronics industry (including computers and equipment, radio, TV and communication devices, medical equipment, precision and optical instruments) have been significantly up since 2013 and are almost at the same level as the automotive industry. 

Next: metal / plastics/ food

Korea: 710 robot/ 10k employees
Singapore: 
Germany: 322 / 10k
Japan: 308/10k
China: 97/10k

----

## Today's robot - Cambrian explosion

| Secondary schools | Uni    |
|-------------------|--------|
| ![TI-Innovator]() | ![RSLK]() |

Notes:
* TI-Innovator rover can be controlled by a calculator
* By the end of the session you will get a quiz and who answer correctly will get a TI-RSLK for free. 

---

## Sensors

---

## Today's sensors

![overview of all available sensors today]()

----

### Today's Proximity sensors - optical

![3 main optical sensing overview]()

----

### Today's sensor - advanced optical w/ DMD

![How DMD improve LiDaR SNR]()

Notes: Summarizing so far, the performance of the LIDAR system is dictated by the SNR of the system. Three factors determine the SNR: the selection of the detector, the amount of shot noise, and the amount of AFE noise. Of these three, the DMD can greatly reduce the shot noise in a scanning LIDAR system by passing the light from the returning signal while rejecting the ambient light which causes the majority of the shot noise. This is possible because the returning signal will come from a certain direction, while the ambient light will pass to the detector from all directions. When using the DMD in the optical system, turning on and off different mirrors corresponds to passing or rejecting light through the optical system. In this case, mirrors are turned on to pass the signal, and other mirrors are turned off to reject the ambient light. Since the DMD can dynamically update the orientation of the micro-mirrors at a very fast update rate, the subset of mirrors on the DMD passing the light can track the scan rate of the laser.

Performance analysis This section discusses how much performance improvement can be realized from using the DMD in a LIDAR system. The DMD is effective enough to reduce the shot noise to a negligible level compared to the other noise components in the system. The capability of the DMD to improve performance of a LIDAR system is directly related to the pattern refresh rate, the contrast, and the optical efficiency of the DMD. TI has many different DMD chipsets that could be used for this application. This paper focuses on the DLP5531-Q1 for automotive applications, but other non-automotive DMDs may be suitable for industrial applications. Since different DMDs have different pattern update rates and optical efficiencies, the performance described here will vary by DMD. Please consult the datasheet or TI with any questions on the performance of a specific DMD chipset.

----

### Today's sensor - SoC mmWave sensor

Include here external link to 2018 radar slide

----

### Today's sensor - SoC mmWave sensor

![Image showing power line radar response]()

Notes: 
https://arxiv.org/pdf/1612.00593.pdf

----

### Autonomous #1 – a lot of sensors

![image showing all sensors ADAS]()

----

### Autonomous #2 – a lot of sensors

![image showing all sensors cobot]()

----

### Autonomous #3 – a lot of sensors

![image showing all sensors AGV]()

----

### Autonomous #4 – how to get started

![image showing reference design]()

Notes:
Reduces RF design challenges and time 
Customers with no RF design experience can design and develop their solution in-house 
Saves 3-6 months of antenna development time
Simplifies manufacturing
Manufacture anywhere worldwide due to simpler and lower cost FR4 design
Minimizes board size
Viable for industrial market applications where footprint needs to be small
75% smaller than 24-GHz 
40% smaller than 60-GHz non-AoP

Cons of antenna on package: lose design flexibility, especially when needing to configure antennas for outdoor use or long distances, i.e., traffic monitoring

---

## Today's programming

----

## Machine Learning Overview

![Tree of all machine learning]()

----

## Why is AI/ML growing in popularity

| ![Pyramid AI]() | ![Timeline AI]() |

Notes: 
Deep Learning: 
The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986,[24][13] and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.[25][26]
The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1965.[27] A 1971 paper described a deep network with 8 layers trained by the group method of data handlingalgorithm.[28]
Other deep learning working architectures, specifically those built for computer vision, began with the Neocognitron introduced by Kunihiko Fukushima in 1980.[29] In 1989, Yann LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970,[30][31][32][33] to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days.[34]
By 1991 such systems were used for recognizing isolated 2-D hand-written digits, while recognizing 3-D objects was done by matching 2-D images with a handcrafted 3-D object model. Weng et al. suggested that a human brain does not use a monolithic 3-D object model and in 1992 they published Cresceptron,[35][36][37] a method for performing 3-D object recognition in cluttered scenes. Because it directly used natural images, Cresceptron started the beginning of general-purpose visual learning for natural 3D worlds. Cresceptron is a cascade of layers similar to Neocognitron. But while Neocognitron required a human programmer to hand-merge features, Cresceptron learned an open number of features in each layer without supervision, where each feature is represented by a convolution kernel. Cresceptron segmented each learned object from a cluttered scene through back-analysis through the network. Max pooling, now often adopted by deep neural networks (e.g. ImageNet tests), was first used in Cresceptron to reduce the position resolution by a factor of (2x2) to 1 through the cascade for better generalization.

----

## What is DL/CNN competing against?

From 2012 when CNN entered ImageNet classification and classical AI dominatedyoy, huge gains can be seen and from 2013 onwards, all top players are CNN based

| Rank | 2012 | % error | 2013 | % error | 2014 | % error |
|------|------|---------|------|---------|------|---------|
| #1   | Supervision (Toronto) | 15.3% | Clarifai | 11.7% | GoogleNet | 6.8% |
| #2   | <span style="blue">ISI (Tokoy)</span> | 26.1% | NUS | 12.9 % | VGG | 7.3 %|
> ImageNet classification (summary fro Yann le Cunn)

Notes:
Blue is old style ML
Red is CNN/Deep Learning
In a few years effectiveness on ImageNet of CNN has taken over the research world and since those problems were very close to industry needs, these have taken over the industry

----

### What is CNN

![image of CNN layers]()
Fig 3. 
> Adapted from doi:10.1038/nature14539

Notes:
Figure 2 | Inside a convolutional network. The outputs (not the filters) of each layer (horizontally) of a typical convolutional network architecture applied to the image of a Samoyed dog (bottom left; and RGB (red, green, blue) inputs, bottom right). Each rectangular image is a feature map corresponding to the output for one of the learned features, detected at each of the image positions. Information flows bottom up, with lower-level features acting as oriented edge detectors, and a score is computed for each image class in output. ReLU, rectified linear unit.

Backpropagation to train multilayer architectures From the earliest days of pattern recognition22,23, the aim of researchers has been to replace hand-engineered features with trainable multilayer networks, but despite its simplicity, the solution was not widely understood until the mid 1980s. As it turns out, multilayer architectures can be trained by simple stochastic gradient descent. As long as the modules are relatively smooth functions of their inputs and of their internal weights, one can compute gradients using the backpropagation procedure. The idea that this could be done, and that it worked, was discovered independently by several different groups during the 1970s and 1980s24–27. The backpropagation procedure to compute the gradient of an objective function with respect to the weights of a multilayer stack of modules is nothing more than a practical application of the chain rule for derivatives. The key insight is that the derivative (or gradient) of the objective with respect to the input of a module can be computed by working backwards from the gradient with respect to the output of that module (or the input of the subsequent module) (Fig. 1). The backpropagation equation can be applied repeatedly to propagate gradients through all modules, starting from the output at the top (where the network produces its prediction) all the way to the bottom (where the external input is fed). Once these gradients have been computed, it is straightforward to compute the gradients with respect to the weights of each module. 

Many applications of deep learning use feedforward neural network architectures (Fig. 1), which learn to map a fixed-size input (for example, an image) to a fixed-size output (for example, a probability for each of several categories). To go from one layer to the next, a set of units compute a weighted sum of their inputs from the previous layer and pass the result through a non-linear function. At present, the most popular non-linear function is the rectified linear unit (ReLU), which is simply the half-wave rectifier f(z)= max(z, 0). In past decades, neural nets used smoother non-linearities, such as tanh(z) or 1/(1+exp(−z)), but the ReLU typically learns much faster in networks with many layers, allowing training of a deep supervised network without unsupervised pre-training28. Units that are not in the input or output layer are conventionally called hidden units. The hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (Fig. 1). In the late 1990s, neural nets and backpropagation were largely forsaken by the machine-learning community and ignored by the computer-vision and speech-recognition communities. It was widely thought that learning useful, multistage, feature extractors with little prior knowledge was infeasible. In particular, it was commonly thought that simple gradient descent would get trapped in poor local minima — weight configurations for which no small change would reduce the average error

----

### AI at the edge vs in the cloud

![AI at the edge vs cloud]()

Notes:
Training is similar to “going to university” to acquire new skills
Inference is similar to “using learned skills” in the workplace

Inference at the edge also offers the possibility to have much smaller reaction time which might be needed by the process.

----

### Embedded CNN approaches

* 




0. Data alignment <!-- .element: class="fragment" data-fragment-index="1" -->
1. Entity assessment (e.g. signal/feature/object). <!-- .element: class="fragment" data-fragment-index="2" -->
2. Tracking and object detection/recognition/identification <!-- .element: class="fragment" data-fragment-index="3" -->
3. Situation assessment <!-- .element: class="fragment" data-fragment-index="4" -->
4. Impact assessment <!-- .element: class="fragment" data-fragment-index="5" -->
5. Process refinement (i.e. sensor management) <!-- .element: class="fragment" data-fragment-index="6" -->
6. User refinement <!-- .element: class="fragment" data-fragment-index="7" -->

---

## ROS

<ul>
   <span class="fragment"><li> 2007 *real* start to avoid constant re-inventing the wheel </li></span>
   <span class="fragment"><li> 2012 <a href="https://rosindustrial.org/briefhistory">ROS-I</a> github launched </li></span>
   <span class="fragment"><li> 2017 <a href="https://index.ros.org/doc/ros2/Releases/">ROS2<a/>, 
      <span class="fragment"><small><a href="https://www.generationrobots.com/blog/en/ros-vs-ros2/">1v2,</a></small></span>
       <span class="fragment"> <small><a href="https://design.ros2.org/articles/why_ros2.html">why</a></small></small> </li>
</ul>
<iframe data-src="https://www.theconstructsim.com/timeline-robot-operating-system-ros/" width="800" height="600" frameborder="0" marginwidth="0" marginheight="0" scrolling="yes" style="border:3px solid #666; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>

Notes: What is ROS, let's do a little look back in the future of robotics. We can also have a look at the official ROS timeline in this small iframe

---

<div style="position: absolute; width: 40%; right: 0; box-shadow: 0 1px 4px rgba(0,0,0,0.5), 0 5px 25px rgba(0,0,0,0.2); background-color: rgba(0, 0, 0, 0.9); color: #fff; padding: 20px; font-size: 20px; text-align: left;">
    <h2>mmWave and vision fusion in ROS </h2>
  <ul> 
    <li> Why re-invent the wheel? </li></span>
    <span class="fragment"><li> When demo already on ROS tutorial </li></span>
    <span class="fragment"><li> Well because TI mmwave has many more built-in features making fusion stronger and faster </li></span>
    <span class="fragment"><li> and because next step is to do fusion with industrial cameras with  </li></span>
  </ul>
</div>

---
