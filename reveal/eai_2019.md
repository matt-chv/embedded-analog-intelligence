# Embedded Analog Intelligence
> Embedded Systems + Analog Sensors/Actuators + Artificial Intelligence

Notes: Embedded systems interface with analog sensors and process information in real-time at the edge with advanced AI algorithms for increased autonomy

---

<!-- .slide: data-background-iframe="../../static/img/embedded-analog-intelligence.jpg"-->

## Introduction

* From traditional industrial robotic systems to today’s latest collaborative robots (or “cobots”), robots rely artificial intelligence (AI) to become “autonomous,” making real-time decisions at the edge.

* This session will discuss "embedded analog intelligence" which is how to benefit from latest sensors, sensor fusion, and most recent deep neural networks to take the best real time decision at the edge.

* Lecture : 
    - language : English 
    - Duration: 30mn

Notes: 
* AI is a component of modern robot

* Key differentiation is:
    - Embedded, real-time
    - Analog sensing and actuation
    - Intelligent / leading edge processing

Embedded Analog Intelligence

---

## Historical point of view

----

## First robot: Unimate a “programmed article transfer”

|UNIMATE PUMA 200 Robot Arm| Patents |
|--|--|
|![]() | ![]() |
| > Picture by Razor Robotics licensed under the Creative Commons Attribution 2.0 Generic on Wikimedia | > Source: https://patents.google.com/patent/US2988237A/en
|
 

Notes: 
George Devol was only 9 years old when the word “robot” first appeared, in 1921, introduced in Karel Capek’s play <a href="http://en.wikipedia.org/wiki/R.U.R._%28Rossum%27s_Universal_Robots%29">R.U.R. (Rossum’s Universal Robots)</a>

The robots in the play had a human form and were manufactured in vats like beer. In contrast, the robots that Devol would invent decades later were electromechanical machines—the first digitally operated programmable robotic arms—and they would start a revolution in manufacturing that continues to this day.</p>   <p><a href="http://en.wikipedia.org/wiki/George_Devol">Devol</a>, who died August 11, 2011, at the age 99, was a prolific inventor and entrepreneur. His work led to the development of the first industrial robot, called Unimate [photo above],

<p>In fact, in 1941 Isaac Asimov coined the word “robotics” in his story “<a href="http://en.wikipedia.org/wiki/Liar!">Liar!</a>” in <em>Astounding Science Fiction</em> magazine. Asimov told me in great detail when we met one evening in New York why he coined the word. He was tired of listing all the activities around robots such as design, construction, operations, manufacturing, etc. He wanted a word to cover all of this. He did not know at the time that Devol was already the living embodiment of robotics.</p>

<p>In 1954, Devol applied for a patent for a device called the Programmed Article Transfer. Looking for an entrepreneurial partner, Devol found one, at a cocktail party, by the name of <a href="http://en.wikipedia.org/wiki/Joseph_Engelberger">Joseph Engelberger</a>, an executive with engineering degrees from Columbia University. Engelberger, who shared an enthusiasm for science fiction with Devol, took the transfer machine to his heart, he told me during an interview in 1977.</p>   <p><img alt="george devol patent" height="343" src="/image/1929980" width="450"></p>   <p><img alt="george devol unimation unimate" height="377" src="/image/1929930" width="450"></p>   <p>Their device morphed from “programmed article transfer” to “manipulator” to “robot.” Devol and Engelberger made this decision to help improve their marketing opportunities. Selling the concept even with a working prototype was an uphill chore. But it paid off:&nbsp;The robot connection gave the project an extra dose of energy that helped it succeed.</p>   <p>The first Unimate, a product of their new Unimation Corp., was hydraulically powered. Its control system relied upon digital control, a magnetic drum memory, and discrete solid-state control components. In 1961 the first Unimate was installed at a GM plant in Trenton, New Jersey, to assist a hot die-casting machine. Unimation would soon develop robots for welding and other applications. <a href="http://www.google.com/patents?id=nBlQAAAAEBAJ&amp;zoom=4&amp;pg=PA13#v=onepage&amp;q&amp;f=false">Patent Number 2,988,237</a> was the seed that spawned the robot industry.</p> 
https://patents.google.com/patent/US2988237A/en

----

## First robot sensors : Electromecanical sensors

| Temp | Position |
|------|----------|
| ![]() | ![]()   |
| Fig 1. | Fig 2. |

Notes: The principle behind a bimetallic strip is that different metals expand to different extents with temperature changes. By combining two different metals one on top of another into a strip, a bimetallic strip is formed. As the two metals expand or contract differently under the same temperature change, the strip bends. It can then be used to switch on or off a circuit at certain temperatures. Bimetallic strips are often found in ovens. The typical structure of this type of control is shown in Fig 1.

Fig 2. illustrate a rotary switch A rotary switch is a switch operated by rotation.  rotary switch consists of a spindle or "rotor" that has a contact arm or "spoke" which projects from its surface like a cam. It has an array of terminals, arranged in a circle around the rotor, each of which serves as a contact for the "spoke" through which any one of a number of different electrical circuits can be connected to the rotor. 

https://blog.robotiq.com/the-history-of-robot-programming-languages
Programming: 
he first real "robotic programming" arrived, unsurprisingly, along with the first industrial robot. The Unimate was invented by George Devol in 1954, who you can learn more about in our previous article. The robot used a very low level of programming (as described in the original patent), which involved "teaching" the robot. The user commanded the robot to a position then stored the positions of all the joints. When the program was played back, the joints would move between positions comparing them to its current position. There was no "language" as such, but this process still happens at the low level of modern robot programming languages.

---

## Today's robots

----

## Today's robot - Cambrian explosion

| Public | Research & Education | Industrial | Commercial | Consumer | 
|--------|-----------------------|----------|-------------|----------|
| ![]() |  ![]() |  ![]() |  ![]() |  ![]() | 
| Defense |  K12 |  Manufacturing/Collaborative | Last mile delivery (wheeled/flying) |  Home & Lawn | 
| Security/Surveillance |  College University | Construction/Demolition |  Inventory/wholesale/retail |  Toy & Hobby | 
| Public safey |  Vocational training |  Oil and Gas |  Hospitality/ Healthcare QoL |  Home Healthcare QoL | 
| Emergency response |  Research |  Mining and Quarrying |  Wharehouse/Distribution |  Social Ententairnment | 
| Ressource Management |  Exploration |  Agriculture |  Tele Presence |  Transportation | 

Notes:
Source: WTWH Media LLC webcast + own research
Key message: after years of increasing penetration – 

https://ifr.org/downloads/press2018/Executive_Summary_WR_2018_Industrial_Robots.pdf
30% CAGR according to IFR
Since 2010, the automotive industry –the most important customer of industrial robots– considerably increased investments in industrial robots worldwide

Robot sales to the electrical/electronics industry (including computers and equipment, radio, TV and communication devices, medical equipment, precision and optical instruments) have been significantly up since 2013 and are almost at the same level as the automotive industry. 

Next: metal / plastics/ food

Korea: 710 robot/ 10k employees
Singapore: 
Germany: 322 / 10k
Japan: 308/10k
China: 97/10k

----

## Today's robot - Cambrian explosion

| Secondary schools | Uni    |
|-------------------|--------|
| ![TI-Innovator]() | ![RSLK]() |

Notes:
* TI-Innovator rover can be controlled by a calculator
* By the end of the session you will get a quiz and who answer correctly will get a TI-RSLK for free. 

---

## Sensors

---

## Today's sensors

![overview of all available sensors today]()

----

### Today's Proximity sensors - optical

![3 main optical sensing overview]()

----

### Today's sensor - advanced optical w/ DMD

![How DMD improve LiDaR SNR]()

Notes: Summarizing so far, the performance of the LIDAR system is dictated by the SNR of the system. Three factors determine the SNR: the selection of the detector, the amount of shot noise, and the amount of AFE noise. Of these three, the DMD can greatly reduce the shot noise in a scanning LIDAR system by passing the light from the returning signal while rejecting the ambient light which causes the majority of the shot noise. This is possible because the returning signal will come from a certain direction, while the ambient light will pass to the detector from all directions. When using the DMD in the optical system, turning on and off different mirrors corresponds to passing or rejecting light through the optical system. In this case, mirrors are turned on to pass the signal, and other mirrors are turned off to reject the ambient light. Since the DMD can dynamically update the orientation of the micro-mirrors at a very fast update rate, the subset of mirrors on the DMD passing the light can track the scan rate of the laser.

Performance analysis This section discusses how much performance improvement can be realized from using the DMD in a LIDAR system. The DMD is effective enough to reduce the shot noise to a negligible level compared to the other noise components in the system. The capability of the DMD to improve performance of a LIDAR system is directly related to the pattern refresh rate, the contrast, and the optical efficiency of the DMD. TI has many different DMD chipsets that could be used for this application. This paper focuses on the DLP5531-Q1 for automotive applications, but other non-automotive DMDs may be suitable for industrial applications. Since different DMDs have different pattern update rates and optical efficiencies, the performance described here will vary by DMD. Please consult the datasheet or TI with any questions on the performance of a specific DMD chipset.

----

### Today's sensor - SoC mmWave sensor

Include here external link to 2018 radar slide

----

### Today's sensor - SoC mmWave sensor

![Image showing power line radar response]()

Notes: 
https://arxiv.org/pdf/1612.00593.pdf

----

### Autonomous #1 – a lot of sensors

![image showing all sensors ADAS]()

----

### Autonomous #2 – a lot of sensors

![image showing all sensors cobot]()

----

### Autonomous #3 – a lot of sensors

![image showing all sensors AGV]()

----

### Autonomous #4 – how to get started

![image showing reference design]()

Notes:
Reduces RF design challenges and time 
Customers with no RF design experience can design and develop their solution in-house 
Saves 3-6 months of antenna development time
Simplifies manufacturing
Manufacture anywhere worldwide due to simpler and lower cost FR4 design
Minimizes board size
Viable for industrial market applications where footprint needs to be small
75% smaller than 24-GHz 
40% smaller than 60-GHz non-AoP

Cons of antenna on package: lose design flexibility, especially when needing to configure antennas for outdoor use or long distances, i.e., traffic monitoring

---

## Today's programming

----

## Machine Learning Overview

![Tree of all machine learning]()

----

## Why is AI/ML growing in popularity

| ![Pyramid AI]() | ![Timeline AI]() |

Notes: 
Deep Learning: 
The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986,[24][13] and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.[25][26]
The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1965.[27] A 1971 paper described a deep network with 8 layers trained by the group method of data handlingalgorithm.[28]
Other deep learning working architectures, specifically those built for computer vision, began with the Neocognitron introduced by Kunihiko Fukushima in 1980.[29] In 1989, Yann LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970,[30][31][32][33] to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days.[34]
By 1991 such systems were used for recognizing isolated 2-D hand-written digits, while recognizing 3-D objects was done by matching 2-D images with a handcrafted 3-D object model. Weng et al. suggested that a human brain does not use a monolithic 3-D object model and in 1992 they published Cresceptron,[35][36][37] a method for performing 3-D object recognition in cluttered scenes. Because it directly used natural images, Cresceptron started the beginning of general-purpose visual learning for natural 3D worlds. Cresceptron is a cascade of layers similar to Neocognitron. But while Neocognitron required a human programmer to hand-merge features, Cresceptron learned an open number of features in each layer without supervision, where each feature is represented by a convolution kernel. Cresceptron segmented each learned object from a cluttered scene through back-analysis through the network. Max pooling, now often adopted by deep neural networks (e.g. ImageNet tests), was first used in Cresceptron to reduce the position resolution by a factor of (2x2) to 1 through the cascade for better generalization.

----

## What is DL/CNN competing against?

From 2012 when CNN entered ImageNet classification and classical AI dominatedyoy, huge gains can be seen and from 2013 onwards, all top players are CNN based

| Rank | 2012 | % error | 2013 | % error | 2014 | % error |
|------|------|---------|------|---------|------|---------|
| #1   | Supervision (Toronto) | 15.3% | Clarifai | 11.7% | GoogleNet | 6.8% |
| #2   | <span style="blue">ISI (Tokoy)</span> | 26.1% | NUS | 12.9 % | VGG | 7.3 %|
> ImageNet classification (summary fro Yann le Cunn)

Notes:
Blue is old style ML
Red is CNN/Deep Learning
In a few years effectiveness on ImageNet of CNN has taken over the research world and since those problems were very close to industry needs, these have taken over the industry

----

### What is CNN

![image of CNN layers]()
Fig 3. 
> Adapted from doi:10.1038/nature14539

Notes:
Figure 2 | Inside a convolutional network. The outputs (not the filters) of each layer (horizontally) of a typical convolutional network architecture applied to the image of a Samoyed dog (bottom left; and RGB (red, green, blue) inputs, bottom right). Each rectangular image is a feature map corresponding to the output for one of the learned features, detected at each of the image positions. Information flows bottom up, with lower-level features acting as oriented edge detectors, and a score is computed for each image class in output. ReLU, rectified linear unit.

Backpropagation to train multilayer architectures From the earliest days of pattern recognition22,23, the aim of researchers has been to replace hand-engineered features with trainable multilayer networks, but despite its simplicity, the solution was not widely understood until the mid 1980s. As it turns out, multilayer architectures can be trained by simple stochastic gradient descent. As long as the modules are relatively smooth functions of their inputs and of their internal weights, one can compute gradients using the backpropagation procedure. The idea that this could be done, and that it worked, was discovered independently by several different groups during the 1970s and 1980s24–27. The backpropagation procedure to compute the gradient of an objective function with respect to the weights of a multilayer stack of modules is nothing more than a practical application of the chain rule for derivatives. The key insight is that the derivative (or gradient) of the objective with respect to the input of a module can be computed by working backwards from the gradient with respect to the output of that module (or the input of the subsequent module) (Fig. 1). The backpropagation equation can be applied repeatedly to propagate gradients through all modules, starting from the output at the top (where the network produces its prediction) all the way to the bottom (where the external input is fed). Once these gradients have been computed, it is straightforward to compute the gradients with respect to the weights of each module. 

Many applications of deep learning use feedforward neural network architectures (Fig. 1), which learn to map a fixed-size input (for example, an image) to a fixed-size output (for example, a probability for each of several categories). To go from one layer to the next, a set of units compute a weighted sum of their inputs from the previous layer and pass the result through a non-linear function. At present, the most popular non-linear function is the rectified linear unit (ReLU), which is simply the half-wave rectifier f(z)= max(z, 0). In past decades, neural nets used smoother non-linearities, such as tanh(z) or 1/(1+exp(−z)), but the ReLU typically learns much faster in networks with many layers, allowing training of a deep supervised network without unsupervised pre-training28. Units that are not in the input or output layer are conventionally called hidden units. The hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (Fig. 1). In the late 1990s, neural nets and backpropagation were largely forsaken by the machine-learning community and ignored by the computer-vision and speech-recognition communities. It was widely thought that learning useful, multistage, feature extractors with little prior knowledge was infeasible. In particular, it was commonly thought that simple gradient descent would get trapped in poor local minima — weight configurations for which no small change would reduce the average error

----

### AI at the edge vs in the cloud

![AI at the edge vs cloud]()

Notes:
Training is similar to “going to university” to acquire new skills
Inference is similar to “using learned skills” in the workplace

Inference at the edge also offers the possibility to have much smaller reaction time which might be needed by the process.

----

### Embedded CNN approaches

* Efficient CNN configurations
    - Select network configurations with high accuracy-to-complexity ratio
    - Network pruning

* Sparse convolutions
    - Drastic reduction in compute required

* 8-bit fixed-point multiplications for inference
    - Reduce DDR bandwidth significantly
    - Improved utilization of the compute capabilities

----

### Sparse CNN

![Image of sparse matrix]()

Notes: 
Convolution is the most computation-costly operation in CNN.
Sparsity is a technique used during training to decrease the number of non-zero weights by zeroing out of small coefficients.
Drive as many of the weights to zero as possible without sacrificing quality
Does not change the structure of network 
The accuracy lost during training can be recovered by fine-tuning the non-zero coefficients.

'Sparse' and 'dense' refer to the number of zero vs. nonzero elements in an array (e.g. vector or matrix). A sparse array is one that contains mostly zeros, and few non-zero entries. A dense array contains mostly nonzeros. 

### 8-bit quantization

Low complexity 
The quantization approach uses a middle ground between complexity and accuracy.
Using 8 bit helps in DDR BW and performance optimizations.  
Dynamic 
Minimum and maximum ranges of various tensors are computed dynamically for all the layers during training time, from a subset of training data.
Signed tensors are to be quantized between -128 to +127, unsigned integer  to be quantized between 0 and 255.
Quantization multiplication factor to be restricted to be a power of 2
Allows range conversion by using only shifts operation.
Shifts are easily incorporated in the convolution itself for convolution layers.

Notes:
The quantization approach used in this work uses a middle ground between complexity and accuracy.  Minimum and maximum ranges of various tensors are computed for all the layers during training time, from a subset of training data.
Signed tensors are to be quantized between -128 to +127, unsigned integer  to be quanmtized between 0 and 255

Quantization multiplication factor to be restricted to be a power of 2 – allows range conversion by using only shifts operarions,   
Shifts are easily incorporated in the convolution itself for convolution layers

### CNN as an alternative to RNN

| a cat | wonderful |
| ![cat-spectrogram]() | ![wonderful](spectrogram) | 

Notes:
Source: https://elvinouyang.github.io/project/training-cnn-for-voice-command/

In our first research stage, we will turn each WAV file into MFCC vector of the same dimension (the files are of the same length). In the first few hidden layers (of either multi-layer perceptron or 1-D convolutional neural net), we plan to turn the MFCC vectors into log probability of phonemes, i.e. the basic building blocks of a pronounced word. We then plan to feed these sequences to a recurrent neural network (either a RNN or a more advanced LSTM) to train and predict the word. The assumption of this approach is that the MFCC values of a sound clip should reflect the nuance sequence in word pronunciation and the the sequence is strictly ordered. Therefore, the sequence should be be used in recurrent neural networks to classify the words.
In our second research stage, we will turn each WAV file into a visual graph (called spectrogram) of the same size. Since the graphical representation of the voice has pixel points of the same scale on two dimensions, we will then apply convolutional layers on the graphs to extract latent graphical patterns from the files. We will then build fully connected layers to link the extracted feature maps to the expected output. It is even possible to feed the extracted features as a sequence to a recurrent layer since the graphical patterns should also be strictly related to time series, but the model might be too complicated for the simple task we are dealing with. The assumption of this approach is that the graphical patterns in the spectrograms of different words pronounced in the WAV files should be typical enough for the convolutional neural network to train on.

----

<!-- .slide: data-background-iframe="https://www.ti.com/tool/TIDEP-01004" data-background-interactive-->

<div style="position: absolute; width: 40%; right: -50%; box-shadow: 0 1px 4px rgba(0,0,0,0.5), 0 5px 25px rgba(0,0,0,0.2); background-color: rgba(0, 0, 0, 0.9); color: #fff; padding: 20px; font-size: 20px; text-align: left;">
    <h2>Deep Learning inference at the edge reference design</h2>
<p>All design files and SW files available.</p>
</div>

---

# Tomorrow

---

## Tomorrow's sensors LiDar

![VCSEL vs lateral laser diode]()

Notes:
see more at https://www.myvcsel.com/technology

---

## Tomorrow's sensor - Hyperspectral

![Light Spectrum]()

----

## Neural Network of small computing element

![Tier of computing]()

Notes:
Notes from : doi:10.3389/fpsyg.2016.00902
Cytoskeleton: communication and cognition in and within cell boundaries
Neural network across cells
Whole tissue / organ complex patterns in organs
Whole organisms

----

## Moving towards a science of ANN

* 1989: it was proved ([paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.441.7873&rep=rep1&type=pdf)) that a neural network has only a single computational layer, but you allow that one layer to have an unlimited number of neurons, with unlimited connections between them, the network will be capable of performing any task you might ask of it.”
* 2018: 
    - A [paper](https://arxiv.org/abs/1705.05502) completed last year, Rolnick and Max Tegmark of the Massachusetts Institute of Technology proved that by increasing depth and decreasing width, you can perform the same functions with exponentially fewer neurons. They showed that if the situation you’re modeling has 100 input variables, you can get the same reliability using either 2100 neurons in one layer or just 210 neurons spread over two layers. They found that there is power in taking small pieces and combining them at greater levels of abstraction instead of attempting to capture all levels of abstraction at once.
    - Another [paper](https://arxiv.org/abs/1810.00393) proved that at a certain point, no amount of depth can compensate for a lack of width.
    
----

## Non-Euclidean CNN

![Non Euclidean images]()

SUMO challenge from SUNCG:
* 59k indoor scenes
* 360 degrees
* 1024x1024 resolution
* 2D semantic
* 3D semantic
* …


Notes:
In the last decade, Deep Learning approaches (e.g. Convolutional Neural Networks and Recurrent Neural Networks) allowed to achieve unprecedented performance on a broad range of problems coming from a variety of different fields (e.g. Computer Vision and Speech Recognition). Despite the results obtained, research on DL techniques has mainly focused so far on data defined on Euclidean domains (i.e. grids). Nonetheless, in a multitude of different fields, such as: Biology, Physics, Network Science, Recommender Systems and Computer Graphics; one may have to deal with data defined on non-Euclidean domains (i.e. graphs and manifolds). The adoption of Deep Learning in these particular fields has been lagging behind until very recently, primarily since the non-Euclidean nature of data makes the definition of basic operations (such as convolution) rather elusive. Geometric Deep Learning deals in this sense with the extension of Deep Learning techniques to graph/manifold structured data.This website represents a collection of materials in the field of Geometric Deep Learning. We collect workshops, tutorials, publications and code, that several differet researchers has produced in the last years. Our goal is to provide a general picture of this new and emerging field, which is rapidly developing in the scientific community, thanks to the broad applicability it presents.
GET STARTED!


https://arxiv.org/pdf/1611.08097.pdf
Computer vision and graphics: The computer vision community has recently shown an increasing interest in working with 3D geometric data, mainly due to the emergence of affordable range sensing technology such as Microsoft Kinect or Intel RealSense. Many machine learning techniques successfully working on images were tried “as is” on 3D geometric data, represented for this purpose in some way “digestible” by standard frameworks, e.g. as range images [98], [99] or rasterized volumes [100], [101]. The main drawback of such approaches is their treatment of geometric data as Euclidean structures. First, for complex 3D objects, Euclidean representations such as depth images or voxels may lose significant parts of the object or its fine details, or even break FILTER FILTER Euclidean CNN Geometric CNN Fig. 5. Illustration of the difference between classical CNN (left) applied to a 3D shape (checkered surface) considered as a Euclidean object, and a geometric CNN (right) applied intrinsically on the surface. In the latter case, the convolutional filters (visualized as a colored window) are deformation-invariant by construction. its topological structure. Second, Euclidean representations are not intrinsic, and vary when changing pose or deforming the object. Achieving invariance to shape deformations, a common requirement in many vision applications, demands very complex models and huge training sets due to the large number of degrees of freedom involved in describing non-rigid deformations (Figure 5, left). In the domain of computer graphics, on the other hand, working intrinsically with geometric shapes is a standard practice. In this field, 3D shapes are typically modeled as Riemannian manifolds and are discretized as meshes. Numerous studies (see, e.g. [102], [103], [104], [105], [106]) have been devoted to designing local and global features e.g. for establishing similarity or correspondence between deformable shapes with guaranteed invariance to isometries.

Furthermore, different applications in computer vision and graphics may require completely different features: for instance, in order to establish feature-based correspondence between a collection of human shapes, one would desire the descriptors of corresponding anatomical parts (noses, mouths, etc.) to be as similar as possible across the collection. In other words, such descriptors should be invariant to the collection variability. Conversely, for shape classification, one would like descriptors that emphasize the subject-specific characteristics, and for example, distinguish between two different nose shapes (see Figure 6). Deciding a priori which structures should be used and which should be ignored is often hard or sometimes even impossible. Moreover, axiomatic modeling of geometric noise such as 3D scanning artifacts turns out to be extremely hard.

By resorting to intrinsic deep neural networks, the invariance to isometric deformations is automatically built into the model, thus vastly reducing the number of degrees of freedom required to describe the invariance class. Roughly speaking, the intrinsic deep model will try to learn ‘residual’ deformations that deviate from the isometric model. Geometric deep learning can be applied to several problems in 3D shape analysis, which can be divided in two classes. First, problems such as local descriptor learning [47], [53] or correspondence learning [48] (see example in the insert IN7), in which the output of the network is point-wise. The inputs to the network are some point-wise features, for example, color texture or simple geometric features such as normals. Using a CNN architecture with multiple intrinsic convolutional layers, it is possible to produce non-local features that capture the context around each point. The second type of problems such as shape recognition require the network to produce a global shape descriptor, aggregating all the local information into a single vector using e.g. the covariance pooling [47].

https://sumochallenge.org/
The SUMO Challenge targets the development of algorithms for comprehensive understanding of 3D indoor scenes from 360° RGB-D panoramas. The target 3D models of indoor scenes include all visible layout elements and objects complete with pose, semantic information, and texture. Algorithms submitted are evaluated at 3 levels of complexity corresponding to 3 tracks of the challenge: oriented 3D bounding boxes, oriented 3D voxel grids, and oriented 3D meshes. SUMO Challenge results will be presented at the 2019 SUMO Challenge Workshop, at CVPR.

----

## Deep Reinforcement Learning

![Deep Reinforcement Learning]()

Notes:
AlphaZero is a computer program or algorithm developed by the Alphabet-owned artificial intelligence research company DeepMind to master go, chess and shogi, by using an approach similar to AlphaGo Zero. On December 5, 2017 the DeepMind team released a preprint introducing AlphaZero, which, within 24 hours, achieved a superhuman level of play in these three games by defeating world-champion programs, Stockfish, elmo, and the 3-day version of AlphaGo Zero, in each case making use of custom tensor processing units (TPUs) that the Google programs were optimized to use

Source: https://bdtechtalks.com/2019/01/28/deepmind-alphastar-ai-starcraft-2/
To reach the level of proficiency it displayed on Thursday, AlphaStar went through more matches than any human can play in an entire lifetime . Having been acquired by Google in 2014, DeepMind had access to its parent company’s vast compute resources for training their AI models. Each of the AI agents in the AlphaStar league was trained with 16 of Google’s powerful TPUs, processors specialized for AI tasks. During a 14-day process, each agent experience 200 years’ worth of games. According to a AMA Reddit thread with DeepMind researchers, each agent played the equivalent of 10 million games.

Next: grippers trained by reinforcement learning:
https://arxiv.org/abs/1709.07857

nstrumenting and collecting annotated visual grasping datasets to train modern machine learning algorithms can be extremely time-consuming and expensive. An appealing alternative is to use off-the-shelf simulators to render synthetic data for which ground-truth annotations are generated automatically. Unfortunately, models trained purely on simulated data often fail to generalize to the real world. We study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular RGB images. We extensively evaluate our approaches with a total of more than 25,000 physical test grasps, studying a range of simulation conditions and domain adaptation methods, including a novel extension of pixel-level domain adaptation that we term the GraspGAN. We show that, by using synthetic data and domain adaptation, we are able to reduce the number of real-world samples needed to achieve a given level of performance by up to 50 times, using only randomly generated simulated objects. We also show that by using only unlabeled real-world data and our GraspGAN methodology, we obtain real-world grasping performance without any real-world labels that is similar to that achieved with 939,777 labeled real-world samples. 

----

### Tomorrow’s control algorithms

![PID w/ CNN]()

Notes:
Reference: http://www.joace.org/uploadfile/2015/0408/20150408032023512.pdf

----

### Federated Learning

![Federated Learning]()

Notes:
Source: https://ai.googleblog.com/2017/04/federated-learning-collaborative.html
Standard machine learning approaches require centralizing the training data on one machine or in a datacenter. And Google has built one of the most secure and robust cloud infrastructures for processing this data to make our services better. Now for models trained from user interaction with mobile devices, we're introducing an additional approach: Federated Learning. Federated Learning enables mobile phones to collaboratively learn a shared prediction model while keeping all the training data on device, decoupling the ability to do machine learning from the need to store the data in the cloud. This goes beyond the use of local models that make predictions on mobile devices (like the Mobile Vision API and On-Device Smart Reply) by bringing model trainingto the device as well.It works like this: your device downloads the current model, improves it by learning from data on your phone, and then summarizes the changes as a small focused update. Only this update to the model is sent to the cloud, using encrypted communication, where it is immediately averaged with other user updates to improve the shared model. All the training data remains on your device, and no individual updates are stored in the cloud. 

----

### Defending against adversarial artificial intelligence

![Adversarial attack]()
> Goodfellow, Ian J.;  Shlens, Jonathon;  Szegedy, Christian Explaining and Harnessing Adversarial Examples eprint arXiv:1412.6572


Notes:
While Industry 4.0 is all about AI, security and IIOT, few talk about security of AI. This little talked about topic allows attacker to use knowledge of AI used for image recognition to inject noise unseen by humans and lead machines to the wrong conclusions. FAIR & John Hopkins Researchers have now made significant progress leading to 50% accuracy (nearly double from previous best score).

About Adversarial Attacks
An adversarial attack consists of subtly modifying an original image in such a way that the changes are almost undetectable to the human eye. The modified image is called an adversarial image, and when submitted to a classifier is misclassified, while the original one is correctly classified. The real-life applications of such attacks can be very serious –for instance, one could modify a traffic sign to be misinterpreted by an autonomous vehicle, and cause an accident. Another example is the potential risk of inappropriate or illegal content being modified so that it;s undetectable by the content moderation algorithms used in popular websites or by police web crawlers.

Adversarial attacks to image classification systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our method achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6% accuracy. A network based on our method was ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018 — it achieved 50.6% classification accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by ∼10%. Code and models will be made publicly available. 

---

### Multiple inputx and mixed data CNN

![multiple inputs CNN]()

Notes:
Source: https://cloud.google.com/blog/products/ai-machine-learning/how-20th-century-fox-uses-ml-to-predict-a-movie-audience

You need to define sub-modules of the network and then somehow merge them and do further processing on the whole data. This is usually done by creating smaller neural networks within the bigger one. For example, you have one sub-network that processes images (say convolutional network) and another one that processes tabular data (say, dense network), then you combine the outputs of both networks and put some layers on top (dense layers are the simplest case). By merging I mean in here operations like concatenation of the outputs, but other operations are possible as well, for example if all the dimensions match you can simply add them.
Recent example of such network was described on the Google Cloud blog (diagram below), where they used three sources of data: raw videos, tabular data on the movies, and tabular data on viewers of the movies, to forecast the movie audience.

---

## Closing words

* Apple → mechanics
* Steam engine → thermodynamics 
* Telescope → optics 
* Airplane → aerodynamics 
* Telecommunication → information theory 
* Intelligence 
    -> Causal Entropic Forces
    -> Moravec's paradox
    - ???

---

# THE END
> thanks for listening, any questions
---

### Back-up

---

### DIKW pyramid

![DIKW]()
